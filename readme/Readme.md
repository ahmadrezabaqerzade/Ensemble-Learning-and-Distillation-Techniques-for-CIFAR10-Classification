The self-distillation method in this project works by using two different classes for the training data, called teacher and student. The teacher class is used to train the first model, while the student class is used to train the second model using the predictions of the first model as additional input. To achieve this, the first model is trained on the teacher class, and its predictions are recorded for the student class. Then, the student class data is trained along with the recorded predictions from the first model to train the second model.

The code for this method is implemented in the SelfDistillation folder, where the SelfDistillation class is defined in the SelfDistillation.py file. In this class, the student model is trained using the training and test data. Then, the predictions of the student model for both the training and test data are recorded, and the second model is trained using this data and the recorded predictions from the first model.

The knowledge distillation method is also implemented in this project. In this method, the student model is trained using the training data and the predictions of the teacher model as additional input. To achieve this, the teacher model is first trained, and its predictions for both the training and test data are recorded. Then, the student model is trained using the training data and the recorded predictions from the teacher model.

The code for this method is implemented in the KnowledgeDistillation folder, where the KnowledgeDistillation class is defined in the KnowledgeDistillation.py file. In this class, the teacher model is first trained using the training and test data. Then, the predictions of the teacher model for both the training and test data are recorded, and the student model is trained using this data and the recorded predictions from the teacher model.

The ensemble learning method is also implemented in this project. In this method, multiple classification models are trained in parallel, and their outputs are combined during prediction to obtain the final result. To achieve this, several different models with different architectures and trained on different training data are trained, and their outputs are combined during prediction.

The code for this method is implemented in the EnsembleLearning folder, where the EnsembleLearning class is defined in the EnsembleLearning.py file. In this class, multiple models with different architectures are trained using different training data. During prediction, the outputs of each of these models are combined to obtain the final result.

Overall, this project demonstrates that these three methods, self-distillation, knowledge distillation, and ensemble learning, can significantly improve the classification accuracy of the CIFAR10 dataset.
